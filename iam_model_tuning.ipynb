{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNRNN(nn.Module):\n",
    "    def __init__(self, num_chars, input_channels=1):\n",
    "        super(CNNRNN, self).__init__()\n",
    "        \n",
    "        # CNN part\n",
    "        self.cnn = nn.Sequential(\n",
    "            # Layer 1\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            \n",
    "            # Layer 2\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            \n",
    "            # Layer 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2)),\n",
    "            \n",
    "            # Layer 4\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2)),\n",
    "            \n",
    "            # Layer 5\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2))\n",
    "        )\n",
    "        \n",
    "        # Calculate CNN output features\n",
    "        self.lstm_hidden_size = 256\n",
    "        \n",
    "        # Fix 1: Add a projection layer to reduce feature dimensionality\n",
    "        # Instead of directly connecting CNN to RNN, add a 1x1 conv to reduce channels\n",
    "        self.feature_projection = nn.Conv2d(256, 256, kernel_size=1, stride=1)\n",
    "        \n",
    "        # Fix 2: Update LSTM to handle the actual feature size\n",
    "        # For a 32px height image, after CNN we'll have:\n",
    "        # Height: 32 -> 16 -> 8 -> 8 -> 8 -> 8 (after all pooling layers)\n",
    "        # So each time step has 256*8 = 2048 features\n",
    "        self.rnn_input_size = 256  # We'll fix this in forward() by reshaping properly\n",
    "        \n",
    "        # RNN part\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.rnn_input_size,\n",
    "            hidden_size=self.lstm_hidden_size,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Final classifier\n",
    "        self.classifier = nn.Linear(self.lstm_hidden_size * 2, num_chars)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # CNN feature extraction\n",
    "        features = self.cnn(x)  # [batch, channels, height, width]\n",
    "        \n",
    "        # Get dimensions\n",
    "        batch_size, channels, height, width = features.size()\n",
    "        \n",
    "        # Fix: Apply 1x1 convolution to reduce features\n",
    "        features = self.feature_projection(features)  # [batch, 256, height, width]\n",
    "        \n",
    "        # Fix: Reshape for RNN - properly handle height dimension\n",
    "        # Option 1: Flatten height into channels\n",
    "        # features = features.view(batch_size, width, channels * height)\n",
    "        \n",
    "        # Option 2: Average across height dimension (recommended)\n",
    "        features = features.mean(dim=2)  # [batch, channels, width]\n",
    "        features = features.permute(0, 2, 1)  # [batch, width, channels]\n",
    "        \n",
    "        # RNN sequence processing\n",
    "        rnn_output, _ = self.lstm(features)  # [batch, width, 2*hidden_size]\n",
    "        \n",
    "        # Project to character space\n",
    "        logits = self.classifier(rnn_output)  # [batch, width, num_chars]\n",
    "        \n",
    "        # Apply log softmax for CTC loss\n",
    "        log_probs = F.log_softmax(logits, dim=2)\n",
    "        \n",
    "        return log_probs\n",
    "    \n",
    "    def predict(self, x):\n",
    "        log_probs = self.forward(x)\n",
    "        predictions = torch.argmax(log_probs, dim=2)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "# Usage example with CTC loss\n",
    "def train_batch(model, criterion, optimizer, images, targets, target_lengths):\n",
    "    \"\"\"Train the model on a batch of data\"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    log_probs = model(images)\n",
    "    \n",
    "    # Prepare for CTC loss\n",
    "    batch_size = log_probs.size(0)\n",
    "    input_lengths = torch.full(size=(batch_size,), fill_value=log_probs.size(1), dtype=torch.long)\n",
    "    \n",
    "    # Compute CTC loss\n",
    "    loss = criterion(log_probs.transpose(0, 1), targets, input_lengths, target_lengths)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "# Decoding function for inference\n",
    "def decode_predictions(predictions, idx_to_char, blank_idx=0):\n",
    "    \"\"\"\n",
    "    Decode the model predictions using CTC best path decoding\n",
    "    \n",
    "    Args:\n",
    "        predictions: Model predictions [batch_size, sequence_length]\n",
    "        idx_to_char: Mapping from index to character\n",
    "        blank_idx: Index of the blank token\n",
    "        \n",
    "    Returns:\n",
    "        List of decoded texts\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for pred in predictions:\n",
    "        # Remove duplicates\n",
    "        collapsed = []\n",
    "        previous = -1\n",
    "        for p in pred:\n",
    "            if p != previous:\n",
    "                collapsed.append(p.item())\n",
    "            previous = p.item()\n",
    "        \n",
    "        # Remove blanks\n",
    "        decoded = [idx_to_char[idx] for idx in collapsed if idx != blank_idx]\n",
    "        results.append(''.join(decoded))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import Levenshtein\n",
    "\n",
    "def visualize_predictions(model, dataloader, char_to_idx, device, num_samples=5):\n",
    "    \"\"\"\n",
    "    Visualize model predictions on sample images\n",
    "    \n",
    "    Args:\n",
    "        model: Trained CNNRNN model\n",
    "        dataloader: DataLoader containing samples to predict\n",
    "        char_to_idx: Character to index mapping\n",
    "        device: Device to run inference on\n",
    "        num_samples: Number of samples to visualize\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    # Create index to char mapping for decoding\n",
    "    idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "    blank_idx = 0  # CTC blank index\n",
    "    \n",
    "    # Get a batch of data\n",
    "    for images, texts, text_lengths in dataloader:\n",
    "        if images is None:\n",
    "            continue\n",
    "            \n",
    "        # Only take the requested number of samples\n",
    "        n = min(num_samples, images.size(0))\n",
    "        images = images[:n].to(device)\n",
    "        texts = texts[:n]\n",
    "        text_lengths = text_lengths[:n]\n",
    "        \n",
    "        # Make predictions\n",
    "        with torch.no_grad():\n",
    "            predictions = model.predict(images)\n",
    "            decoded_preds = decode_predictions(predictions, idx_to_char, blank_idx)\n",
    "        \n",
    "        # Extract ground truth texts\n",
    "        ground_truths = []\n",
    "        for i, length in enumerate(text_lengths):\n",
    "            gt = ''.join([idx_to_char.get(texts[i, j].item(), '') for j in range(length) if texts[i, j] > 0])\n",
    "            ground_truths.append(gt)\n",
    "        \n",
    "        # Calculate character error rate for each sample\n",
    "        cers = []\n",
    "        for pred, gt in zip(decoded_preds, ground_truths):\n",
    "            cer = Levenshtein.distance(pred, gt) / max(len(gt), 1)\n",
    "            cers.append(cer)\n",
    "        \n",
    "        # Display images with predictions\n",
    "        plt.figure(figsize=(15, 4 * n))\n",
    "        \n",
    "        for i in range(n):\n",
    "            plt.subplot(n, 1, i+1)\n",
    "            \n",
    "            # Get and display image (remove channel dimension and transpose if needed)\n",
    "            img = images[i, 0].cpu().numpy()\n",
    "            plt.imshow(img, cmap='gray')\n",
    "            \n",
    "            # Display prediction and ground truth\n",
    "            correct = decoded_preds[i] == ground_truths[i]\n",
    "            color = 'green' if correct else 'red'\n",
    "            \n",
    "            title = f\"Ground truth: '{ground_truths[i]}'\\n\"\n",
    "            title += f\"Prediction: '{decoded_preds[i]}'\\n\"\n",
    "            title += f\"CER: {cers[i]:.2f}\"\n",
    "            \n",
    "            plt.title(title, color=color)\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Only process one batch\n",
    "        break\n",
    "\n",
    "# Usage\n",
    "def check_trained_model(model, test_loader, char_to_idx, device, num_samples=5):\n",
    "    # Ensure model is loaded properly and on the right device\n",
    "    model = model.to(device)\n",
    "    # Visualize predictions\n",
    "    visualize_predictions(model, test_loader, char_to_idx, device, num_samples)\n",
    "    \n",
    "# Example call:\n",
    "# check_trained_model(trained_model, test_loader, char_to_idx, device, num_samples=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNRNN(\n",
       "  (cnn): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): ReLU()\n",
       "    (15): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (16): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (18): ReLU()\n",
       "    (19): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (feature_projection): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (lstm): LSTM(256, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (classifier): Linear(in_features=512, out_features=77, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = CNNRNN(num_chars=77)\n",
    "base_model.load_state_dict(torch.load('model_weights/base_model_weight.pt'))\n",
    "base_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
